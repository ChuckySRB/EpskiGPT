{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизација\n",
    "\n",
    "> Токенизација је процес представљања сирових података у виду токена (најчеће бајтови података)\n",
    "\n",
    "Од квалитета токена доста зависи и учинак самог модела\n",
    "Токен је најмањи вид податка коме модел даје значење, и пошто их може бити ограничен број, јер цена модела расте значајно са повећањем самог вокабулара, избор величине токена и шта тај токен треба да обухвати је доста тежак задатак, и мале одлуке доста могу утицати на то шта ће модел разумети из података. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизација текста\n",
    "\n",
    "Код токенизације текста, најчешће се текст у виду стрига прво претвори у бајтове помоћу utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здраво b'\\xd0\\x97\\xd0\\xb4\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbe'\n",
      "6 12\n"
     ]
    }
   ],
   "source": [
    "text = \"Здраво\" \n",
    "text_bytes = text.encode(\"utf-8\")\n",
    "\n",
    "print(text, text_bytes)\n",
    "print(len(text), len(text_bytes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Међутим овде настаје проблем, пошто је utf-8 погодан само за латинична слова, __ћирилица__ се претвори у 2 бајта и због алгоритма који користимо за спајање токена, број токена које будемо генерисали ће бити дупло већи и сами токени ће бити мање ефикасни\n",
    "\n",
    "- Због овог разлога ћемо користити сам текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здраво [1047, 1076, 1088, 1072, 1074, 1086]\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "text = \"Здраво\"\n",
    "text_ints = [ord(s) for s in text]\n",
    "\n",
    "print (text, text_ints)\n",
    "print (len(text), len(text_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "\n",
    "Овај алгоритам и две методе које се понављају\n",
    "\n",
    "1. Пронађи два суседна слова која се понављају најчешће\n",
    "\n",
    "2. Замини да два пара са новим токеном\n",
    "\n",
    "Ове две методе се понављају над текстом за тренинг токенизера док се не добије жељени број токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'â', 14: 'ê', 15: 'ô', 16: '̓', 17: 'Ђ', 18: 'Ј', 19: 'Љ', 20: 'Њ', 21: 'Ћ', 22: 'Џ', 23: 'А', 24: 'Б', 25: 'В', 26: 'Г', 27: 'Д', 28: 'Е', 29: 'Ж', 30: 'З', 31: 'И', 32: 'К', 33: 'Л', 34: 'М', 35: 'Н', 36: 'О', 37: 'П', 38: 'Р', 39: 'С', 40: 'Т', 41: 'У', 42: 'Ф', 43: 'Х', 44: 'Ц', 45: 'Ч', 46: 'Ш', 47: 'а', 48: 'б', 49: 'в', 50: 'г', 51: 'д', 52: 'е', 53: 'ж', 54: 'з', 55: 'и', 56: 'к', 57: 'л', 58: 'м', 59: 'н', 60: 'о', 61: 'п', 62: 'р', 63: 'с', 64: 'т', 65: 'у', 66: 'ф', 67: 'х', 68: 'ц', 69: 'ч', 70: 'ш', 71: 'ђ', 72: 'ј', 73: 'љ', 74: 'њ', 75: 'ћ', 76: 'џ', 77: '–', 78: '—', 79: '’', 80: '“', 81: '”', 82: '„'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {} # Вокабулар (int) -> ('char')\n",
    "\n",
    "INPUT_FILE = \"D:\\Caslav\\Poso\\AI\\EpskiGPT\\data\\\\narodne_pesme.txt\"\n",
    "\n",
    "# Функција за учитавањње почетног скупа знакова\n",
    "def create_vocabulary(text: str):\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab = { i:ch for i,ch in enumerate(chars) }\n",
    "    encoder = { ch:i for i,ch in enumerate(chars) }\n",
    "    return vocab, encoder\n",
    "\n",
    "# Отварање фајла из ког вадимо текст\n",
    "with open(INPUT_FILE, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "vocabulary, _ = create_vocabulary(text)\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2): 4, (2, 3): 1, (3, 1): 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Поделићемо тренирање Токенајзера у неколико корака\n",
    "\n",
    "# Помоћна функција која броји понаљање узастопних токена\n",
    "def count_conseq_tokens(ids, counts={}):\n",
    "    \"\"\"\n",
    "    За дату листу интиџера, врати речник броја понављања узаступних парова\n",
    "    Пример: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Опционо ажурира већ дата пребројавања\n",
    "    \"\"\"\n",
    "    for pair in zip(ids[:-1], ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "count_conseq_tokens([1, 2, 3, 1, 2], {(1, 2): 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функција која обацује нове токене у текст\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    Замени све парове pair који се појављују у ids листи са idx\n",
    "    Пример: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    ids_n = len(ids)\n",
    "    if ids_n <= 1 or not pair:\n",
    "        return ids\n",
    "    while i < ids_n-1:\n",
    "        if (ids[i], ids[i+1]) == pair:\n",
    "            new_ids.append(idx)\n",
    "            i+=1\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "        i+=1\n",
    "    \n",
    "    return new_ids\n",
    "\n",
    "merge([4], (), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Једна од техника која се користи код токенизације пре спајања самих слова је подела текста на логичке делове помоћу регекса.\n",
    "Како би се избегли случајеви где се спајају слова и бројеви, делови текста из суседних речи, слова и знакови, итд, користе се техника поделе текста на речи и остале знакове, тако да токенизација буде логичнија. Конкретно GPT користи специфичне регексе који деле текст на логичке целине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ко',\n",
       " ' сме',\n",
       " ' тај',\n",
       " ' може',\n",
       " ',',\n",
       " ' ко',\n",
       " ' не',\n",
       " ' зна',\n",
       " ' за',\n",
       " ' страх',\n",
       " ' тај',\n",
       " ' иде',\n",
       " ' напред',\n",
       " '!',\n",
       " ' -',\n",
       " ' Живојин',\n",
       " '123',\n",
       " '456']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример за ћирилицу\n",
    "\n",
    "primer = \"Ко сме тај може, ко не зна за страх тај иде напред! - Живојин123456\"\n",
    "\n",
    "re.findall(re.compile(GPT4_SPLIT_PATTERN), primer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Као што видите, речи су одвојене у формату размак + реч, знакови су издвојени посебно а бројви такође посебно и то највише до 3 заједно за GPT4\n",
    "- Сада када смо написали неке основне функције можемо да пробамо да направимо најједноставнији токенајзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[[3, 4, 6, 16, 5, 8, 9, 11, 6, 5, 6, 8], [0, 5, 6, 8, 5, 11, 6], [0, 5, 6, 14, 5, 11, 6, 14, 5, 11, 11, 6], [0, 16, 7, 18, 16], [0, 5, 7, 6], [0, 5, 7, 6, 14], [0, 17, 18, 13, 14, 5, 11, 12, 13], [0, 14, 5, 13, 12, 14, 5], [0, 13, 5, 13, 5], [0, 12, 13, 6], [0, 5], [0, 11, 6, 5, 11, 6], [1, 5, 6, 11, 5], [2, 6, 5, 10, 11, 6], [0, 5, 15], [0, 5, 11]]\n",
      "[[3, 4, 6, 16, 5, 8, 9, 19, 5, 6], [0, 5, 6, 8, 5, 19], [0, 5, 6, 14, 5, 19, 14, 5, 11, 19], [0, 16, 7, 18], [0, 5, 7], [0, 5, 7, 6], [0, 17, 18, 13, 14, 5, 11, 12], [0, 14, 5, 13, 12, 14], [0, 13, 5, 13], [0, 12, 13], [0], [0, 19, 5, 19], [1, 5, 6, 11], [2, 6, 5, 10, 19], [0, 5], [0, 5]]\n",
      "[[3, 4, 6, 16, 5, 8, 9, 19, 5], [20, 6, 8, 5], [20, 6, 14, 5, 19, 14, 5, 11], [0, 16, 7], [20], [20, 7], [0, 17, 18, 13, 14, 5, 11], [0, 14, 5, 13, 12], [0, 13, 5], [0, 12], [0], [0, 19, 5], [1, 5, 6], [2, 6, 5, 10], [20], [20]]\n",
      "[[3, 4, 6, 16, 5, 8, 9, 19], [20, 6, 8], [20, 6, 21, 19, 21], [0, 16], [20], [20], [0, 17, 18, 13, 21], [0, 21, 13], [0, 13], [0], [0], [0, 19], [1, 5], [2, 6, 5], [20], [20]]\n",
      "[[3, 4, 6, 16, 5, 8, 9], [22], [22, 21, 19], [0], [20], [20], [0, 17, 18, 13], [0, 21], [0], [0], [0], [0], [1], [2, 6], [20], [20]]\n",
      "[[23, 6, 16, 5, 8], [22], [22, 21], [0], [20], [20], [0, 17, 18], [0], [0], [0], [0], [0], [1], [2], [20], [20]]\n",
      "[[24, 16, 5], [22], [22], [0], [20], [20], [0, 17], [0], [0], [0], [0], [0], [1], [2], [20], [20]]\n"
     ]
    }
   ],
   "source": [
    "# Тренирање токенајзера\n",
    "# Свака епоха замени један токен као најчешће понављани пар\n",
    "def train(text, vocab_size):\n",
    "    # Направи почетни вокабулар\n",
    "    vocab, encoder = create_vocabulary(text)\n",
    "    \n",
    "    # Подели на делове са регексом\n",
    "    text_chunked = re.findall((re.compile(GPT4_SPLIT_PATTERN)), text)\n",
    "\n",
    "    # Преведи текст у почетне токене\n",
    "    tokens = []\n",
    "    for chunk in text_chunked:\n",
    "        tokens.append([encoder[slovo] for slovo in chunk])\n",
    "\n",
    "    # За сваки регекс изброј counts понављања\n",
    "    i = len(vocab.keys())\n",
    "    print(i)\n",
    "    while i < vocab_size:\n",
    "        counts = {}\n",
    "\n",
    "        # Извући прој понављања\n",
    "        for chunk in tokens:\n",
    "            counts = count_conseq_tokens(chunk, counts)\n",
    "        # Извлачи највећи пар\n",
    "        print(tokens)\n",
    "        pair = max(counts, key=counts.get)\n",
    "\n",
    "        # Замени најбројнији пар\n",
    "        tokens = [merge(chunk, pair, i) for chunk in tokens]\n",
    "\n",
    "        vocab[i] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        \n",
    "        # Повећај бројач\n",
    "        i+=1\n",
    "    \n",
    "    # Додај специјалне токене\n",
    "     \n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = train(\"АКдјаклсдадк адкасд адхасдхассд јињј аид аидх љњфхасуф хафуха фафа уфд а сдасд,адса.даосд ач ас\", 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сада када смо истренирали токенајзер остале су још две методе, encode() и decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преводи листу интиџера (токени) у стринг (оригинални текст)\n",
    "def decode(tokens: list[int]) -> str:\n",
    "    text:str = \"\"\n",
    "    \n",
    "    # 1) претварање токена од највећег ка најмањем [vocab_size] -> [0]\n",
    "\n",
    "    i = len(vocab.keys()) - 1\n",
    "    reverse = {v:k for k, v in vocab}\n",
    "    while i >= 0:\n",
    "        pair = vocab[i]\n",
    "        j = len(tokens)\n",
    "        while j < len(tokens):\n",
    "            if (tokens[j]) == i:\n",
    "                tokens = tokens[:j] + [reverse[pair[0]]] + [reverse[pair[1]]] + tokens[j+2:]\n",
    "            j+=1\n",
    "        i-=1\n",
    "\n",
    "    for c in tokens:\n",
    "        text = text + c\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    tokens = []\n",
    "    # 1) регекс модела\n",
    "\n",
    "    # 2) претварање текста по вокабулару од [0] -> [vocab_size]\n",
    "\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
